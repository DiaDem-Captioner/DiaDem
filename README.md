# <img src="assets/diadem.ico" alt="DiaDem icon" width="32px" height="32px" style="position: relative; top: 4px;"> DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models

<p align="left">
  <a href="https://diadem-captioner.github.io/"><img src="https://img.shields.io/badge/Project%20webpage-558b2f?style=for-the-badge"></a>&nbsp;
  <a href="https://huggingface.co/DiaDem-Captioner/DiaDem"><img src="https://img.shields.io/badge/Model-db8905?style=for-the-badge"></a>&nbsp;
  <a href="https://huggingface.co/datasets/DiaDem-Captioner/DiaDemBench"><img src="https://img.shields.io/badge/Benchmark-db8905?style=for-the-badge"></a>&nbsp;
  <a href="https://arxiv.org/abs/2601.19267"><img src="https://img.shields.io/badge/arXiv-red?style=for-the-badge"></a>
</p>

## üìñ Overview
Accurate dialogue description is a critical yet underexplored aspect of audiovisual video captioning, with profound implications for downstream multimodal understanding and generation tasks. Despite the rapid progress in MLLMs, existing approaches often struggle to faithfully capture *who says what* in complex audiovisual scenes. To mitigate this limitation, we propose **DiaDem**, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions, while maintaining strong overall captioning performance across general audiovisual content.

To enable systematic evaluation of dialogue description capabilities, we further introduce **DiaDemBench**, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal that even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.

## üîç Captioning Case of DiaDem
<div align="center">
<img src="assets/case_1.jpg" alt="DiaDem caption" style="width:100%; display: block; margin: 0 auto;">
</div>

We present an illustration of an audiovisual video caption with accurate dialogue descriptions generated by DiaDem, featuring both **correct speaker attribution** and *precise utterance transcription*, as well as other <u>general audiovisual details</u>.

## üèÜ Key Features of DiaDemBench
- üåü **First-of-its-Kind Benchmark:** The first dedicated benchmark to evaluate the accuracy of dialogue descriptions in audiovisual video captioning, focusing on both *correct speaker attribution* and *precise utterance transcription*.
- üìê **Robust Evaluation Protocol:** A principled evaluation framework consisting of ASR (utterance transcription accuracy) and REF (speaker reference accuracy) scores, incorporating a novel adaptive merging strategy for dialogue tuple matching and an MLLM-based judge for verifying speaker consistency.
- üíé **High-Quality Annotation:** A hybrid annotation pipeline in which initial dialogue descriptions are generated using Gemini-2.5-Pro, followed by meticulous manual refinement to ensure accurate utterance transcriptions and reliable speaker attribution.
- üåà **Comprehensive and Diverse Scenarios:** A collection of 1,039 videos covering a wide spectrum of dialogue-centric scenarios, with broad category coverage and balanced distributions, enabling robust and generalizable evaluation.

## üìä Data Statistics of DiaDemBench
<div align="center">
<img src="assets/statistics.jpg" alt="DiaDemBench statistics" style="width:80%; display: block; margin: 0 auto;">
</div>

DiaDemBench features relatively balanced distribution of speaker count, on-screen people count, video duration, and language diversity, while carefully modulating the difficulty of speaker attribution and utterance transcription.

## üß© Categories Remaining Challenging for SOTA Audiovisual Captioning Models
<div align="center">
<img src="assets/challenging_cases.jpg" alt="Challenging cases" style="width:80%; display: block; margin: 0 auto;">
</div>

We showcase four representative dialogue scenarios from DiaDemBench that remain challenging for existing state-of-the-art audiovisual video captioning models to produce accurate dialogue descriptions, with the aim of providing insights for future advancements in audiovisual captioning.

---

## üõ†Ô∏è Inference with DiaDem
Follow the steps below to set up and run DiaDem locally:
```bash
git clone https://github.com/DiaDem-Captioner/DiaDem.git
cd DiaDem
conda env create -f environment.yml
conda activate DiaDem
```

### ‚ö° Quick Usage

Generate a dialogue-aware audiovisual video caption for a single video:
```python
python inference.py <video_path>
```

### üìà Benchmark Evaluation
We also provide scripts to reproduce the results reported in our paper for general audiovisual captioning tasks.

#### Evaluation on video-SALMONN2-testset
```bash
bash eval_scripts/video-SALMONN2-testset/eval_video-SALMONN2-test.sh <your_save_directory>
```

#### Evaluation on UGC-VideoCap
```bash
bash eval_scripts/UGC-VideoCap/eval_UGC-VideoCap.sh <your_save_directory>
```

---

## üöÄ Quick Start of DiaDemBench
Before using DiaDemBench, please carefully read and agree to the following license terms.

### üìú License

DiaDemBench is released under the CC-BY-NC-SA-4.0 license.

> ‚ö†Ô∏è **Important Notice**<br>
> DiaDemBench is intended **for research purposes only** and must not be used for any commercial or other non-research purposes. Users assume full responsibility for any consequences arising from unauthorized use or redistribution.
>
> We do not claim copyright ownership of any raw video files. Video access is provided to researchers under the condition of compliance with the above license. We fully respect and acknowledge the copyrights of the original video creators.
>
> If the original authors request removal of any videos, please contact us by [email](diadem.captioner@gmail.com) or submit an issue.

Once you have agreed to the license terms, follow the steps below to evaluate the dialogue description capability of your audiovisual video captioning model:

1. Download and Prepare DiaDemBench

    Obtain videos and annotations from [HuggingFace](https://huggingface.co/DiaDem-Captioner/DiaDemBench).
    ```bash
    cd DiaDemBench
    unzip videos.zip
    ```

2. Prepare Your Model Predictions

    Save the audiovisual captions generated by your model in a `.jsonl` file, where each line is a JSON object with the following format:
    ```json
    {"video_id": 123, "pred": "the predicted audiovisual caption"}
    ```

3. Run Evaluation

    Execute the scoring script to calculate the REF and ASR scores.
    ```bash
    bash eval_DiaDemBench/run_scoring.sh <your_caption_results_path> <gt_path> <video_dir>
    ```
    - `<your_caption_results_path>`: Path to your `.jsonl` prediction file

    - `<gt_path>`: Path to the ground-truth of DiaDemBench (DiaDemBench/annotations.jsonl)

    - `<video_dir>`: Directory containing the video files of DiaDemBench (DiaDemBench/videos)

---

## üñäÔ∏è Citation

If you find DiaDem or DiaDemBench helpful for your research, please consider giving this repo a star ‚≠ê and citing our paper. We appreciate your support!

```bibtex
@misc{chen2026diademadvancingdialoguedescriptions,
      title={DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models}, 
      author={Xinlong Chen and Weihong Lin and Jingyun Hua and Linli Yao and Yue Ding and Bozhou Li and Bohan Zeng and Yang Shi and Qiang Liu and Yuanxing Zhang and Pengfei Wan and Liang Wang and Tieniu Tan},
      year={2026},
      eprint={2601.19267},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2601.19267}, 
}
```